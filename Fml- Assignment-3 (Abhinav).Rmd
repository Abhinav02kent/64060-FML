---
title: "Fml_assignment_3"
author: "Abhinav"
date: "2024-03-10"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
UN_bank <- read.csv("/Users/keerthanavonteddu/Desktop/abhinav/UniversalBank.csv")
summary(UN_bank)
```


```{r}
library(dplyr)
```

```{r}
library(ISLR)
```

```{r}
library(e1071)
```

```{r}
library(caret)
```

```{r}
library(class)
```

```{r}
library(ggplot2)
```

```{r}
library(tidyr)
```

```{r}
library(gmodels)
```

```{r}
library(lattice)
```


```{r}
UN_bank$Personal.Loan <- factor(UN_bank$Personal.Loan)
UN_bank$Online <- factor(UN_bank$Online)
UN_bank$CreditCard <- factor(UN_bank$CreditCard)
```


# 1.
```{r}
set.seed(231)
Index_train <- createDataPartition(UN_bank$Personal.Loan,p = 0.6,list = FALSE)
Train_df <- UN_bank[Index_train,]
validation.df <- UN_bank[-Index_train,]
```



```{r}
pivot_table <- xtabs(~ CreditCard + Online + Personal.Loan,data = Train_df)
ftable(pivot_table)
```

# 2. 
```{r}
probabality = 48/(48+474)
probabality
```

# 3. 
```{r}
table(Personal.Loan = Train_df$Personal.Loan, Online = Train_df$Online)
table(Personal.Loan = Train_df$Personal.Loan, CreditCard = Train_df$CreditCard)
table(Personal.Loan = Train_df$Personal.Loan)
```

# 4. 
# (i)
```{r}
Probabality.1 <- 90/(90+198)
Probabality.1
```


# (ii)
```{r}
Probabality.2 <- 165/(165+123)
Probabality.2
```

# (iii)
```{r}
Probabality.3 <- 288/(288+2712)
Probabality.3
```

# (iv)
```{r}
Probabality.4 <- 782/(782+1930)
Probabality.4
```

# (v)
```{r}
Probabality.5 <- 1624/(1624+1088)
Probabality.5
```

# (vi)
```{r}
Probability.6 <- 2712/(2712+288)
Probability.6
```


# 5. 
```{r}
calculated_probabilities <- (Probabality.1 * Probabality.2 * Probabality.3)/((Probabality.1 * Probabality.2 * Probabality.3) + (Probabality.4 * Probabality.5 * Probability.6))
calculated_probabilities
```

# 6.
We obtained a #Value of 0.09195402 from question 2, which is nearly identical to the value of 0.09918921 from question 5. The precise approach and the naÃ¯ve bayes technique are identical in every way, however the former requires the precise categorization of every independent variable for prediction, while the latter does not. The figure obtained from question 2 is more accurate, as we can validate. Considering that the pivot table's precise values were used.


# 7.
```{r}
naviebayes.model <- naiveBayes(Personal.Loan ~ Online + CreditCard, data = Train_df)
to.predict = data.frame(Online=1, CreditCard= 1)
predict(naviebayes.model, to.predict,type = 'raw')
```

#The value we obtained from task 5 is 0.09918921, and the value we get from question 7 is 0.9070972. The outcome is nearly identical to what we obtained from task 5. #The rounding results in a very little change. The output's rank order will remain unaffected by the discrepancy.



























































































